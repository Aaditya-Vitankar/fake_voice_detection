{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-25 15:23:21.575999: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-25 15:23:21.579343: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-25 15:23:21.627663: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-25 15:23:21.627700: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-25 15:23:21.628693: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-25 15:23:21.635364: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-25 15:23:21.636043: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-25 15:23:22.607303: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from main import *\n",
    "import main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sudo apt-get install wget\n",
    "mkdir /tmp/mysql_workbench_install && cd /tmp/mysql_workbench_install\n",
    "wget https://dev.mysql.com/get/Downloads/MySQLGUITools/mysql-workbench-community_8.0.23-1ubuntu20.10_amd64.deb #latest release (13-02-21)\n",
    "sudo dpkg -i *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mysql'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmysql\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mysql'"
     ]
    }
   ],
   "source": [
    "import mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mysql'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmysql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnector\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mconnection\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     mydb \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mconnect(host\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocalhost\u001b[39m\u001b[38;5;124m\"\u001b[39m,database\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpuneet123\u001b[39m\u001b[38;5;124m\"\u001b[39m,user\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\"\u001b[39m,password\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPuneet@123\u001b[39m\u001b[38;5;124m\"\u001b[39m,use_pure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mysql'"
     ]
    }
   ],
   "source": [
    "import mysql.connector as connection\n",
    "\n",
    "try:\n",
    "    mydb = connection.connect(host=\"localhost\",database=\"puneet123\",user=\"root\",password=\"Puneet@123\",use_pure=True)\n",
    "\n",
    "    query = \"show databases\"\n",
    "\n",
    "    cursor = mydb.cursor()\n",
    "    cursor.execute(query)\n",
    "    # mydb.commit()\n",
    "    print(cursor.fetchall())\n",
    "\n",
    "except Exception as e:\n",
    "    mydb.close()\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing REAL files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dai/anaconda3/lib/python3.11/site-packages/librosa/core/spectrum.py:256: UserWarning: n_fft=2048 is too large for input signal of length=66\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing FAKE files...\n"
     ]
    }
   ],
   "source": [
    "df = create_dataset(audio_dir=\"../Project/DATA/KAGGLE/AUDIO\" , segment_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./DATA/ALL_DATA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-29 11:33:17.018011: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-29 11:33:17.068572: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-29 11:33:17.068612: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-29 11:33:17.069894: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-29 11:33:17.077981: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-29 11:33:18.094876: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, '../Project/Resources')\n",
    "\n",
    "import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing REAL files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dai/Desktop/Puneet_project/Project/../Project/Resources/lib.py:54: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(file_path)\n",
      "/home/dai/anaconda3/envs/DNN/lib/python3.11/site-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing ../Project/DATA/LSTM Train Test Split/Audio/train/REAL/biden-original.wav: \n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/biden-original.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/obama-original.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/margot-original.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/ryan-original.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/linus-original.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/musk-original.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/taylor-original.csv\n",
      "Processing FAKE files...\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/ryan-to-taylor.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/biden-to-taylor.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/musk-to-margot.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/linus-to-trump.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/taylor-to-trump.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/biden-to-Obam.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/taylor-to-linus.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/margot-to-musk.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/linus-to-margot.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/musk-to-taylor.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/obama-to-ryan.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/biden-to-musk.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/linus-to-biden.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/Obama-to-Trump.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/biden-to-linus.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/musk-to-ryan.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/obama-to-margot.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/linus-to-musk.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/ryan-to-linus.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/margot-to-obam.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/margot-to-linus.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/biden-to-Trump.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/musk-to-biden.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/taylor-to-margot.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/Obama-to-Biden.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/biden-to-margot.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/ryan-to-margot.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/musk-to-trump.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/margot-to-biden.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/ryan-to-trump.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/taylor-to-biden.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/linus-to-ryan.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/margot-to-taylor.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/musk-to-obam.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/biden-to-ryan.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/margot-to-trump.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/ryan-to-biden.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/ryan-to-musk.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/taylor-to-ryan.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/obama-to-musk.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/obama-to-linus.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/linus-to-taylor.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/taylor-to-obam.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/margot-to-ryan.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/taylor-to-musk.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/linus-to-obam.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/ryan-to-obam.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/musk-to-linus.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/train_csv/obama-to-taylor.csv\n"
     ]
    }
   ],
   "source": [
    "lib.create_dataset_sep(audio_dir='../Project/DATA/LSTM Train Test Split/Audio/train' , segment_length=1,\n",
    "                       csv_output_path = './DATA/LSTM Train Test Split/train_csv/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing REAL files...\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/test_csv/trump-original.csv\n",
      "Processing FAKE files...\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/test_csv/trump-to-ryan.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/test_csv/trump-to-Biden.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/test_csv/trump-to-margot.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/test_csv/trump-to-taylor.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/test_csv/trump-to-Obam.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/test_csv/trump-to-linus.csv\n",
      "Dataset created and saved to ./DATA/LSTM Train Test Split/test_csv/trump-to-musk.csv\n"
     ]
    }
   ],
   "source": [
    "lib.create_dataset_sep(audio_dir='../Project/DATA/LSTM Train Test Split/Audio/test' , segment_length=1,\n",
    "                       csv_output_path = './DATA/LSTM Train Test Split/test_csv/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
