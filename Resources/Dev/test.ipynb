{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "###-----------------\n",
    "### Import Libraries\n",
    "###-----------------\n",
    "\n",
    "import os\n",
    "import logging\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections.abc import Callable\n",
    "from typing import Literal\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.autograd import Variable\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# for Frature Extraction\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###----------------\n",
    "### Some parameters\n",
    "###----------------\n",
    "\n",
    "inpDir = '../../DATA/KAGGLE'\n",
    "outDir = '../../DATA'\n",
    "subDir = 'csvs'\n",
    "audDir = \"AUDIO\"\n",
    "\n",
    "RANDOM_STATE = 24               # REMEMBER: to remove at the time of promotion to production\n",
    "np.random.seed(RANDOM_STATE)    # Set Random Seed for reproducible  results\n",
    "\n",
    "EPOCHS = 11                     # number of epochs\n",
    "ALPHA = 0.001                   # learning rate\n",
    "NUM_SAMPLES = 1280              # How many samples we want to generate \n",
    "NOISE = 0.2                     # Noise to be introduced in the data\n",
    "TEST_SIZE = 0.2\n",
    "BATCH_SIZE = 26\n",
    "PATIENCE = 20\n",
    "\n",
    "# parameters for Matplotlib\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 8),\n",
    "          'axes.labelsize': 'x-large',\n",
    "          'axes.titlesize':'x-large',\n",
    "          'xtick.labelsize':'x-large',\n",
    "          'ytick.labelsize':'x-large'\n",
    "         }\n",
    "\n",
    "CMAP = 'coolwarm' # plt.cm.Spectral\n",
    "\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' to Extract feature from audio params:\n",
    "        file_path : <abs. path of the file(.wav format recoomended)>\n",
    "        segment_length : inverl for taking samples\n",
    "'''\n",
    "def extract_features(file_path, segment_length):   # Function to extract features from an audio file\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        y, sr = librosa.load(file_path) \n",
    "        #  Loading audio files returns \n",
    "        # y[audio time series. Multi-channel is supported]\n",
    "        # sr[sampling rate of y] Note: Taking default 22050\n",
    "        # [For more details : https://librosa.org/doc/0.10.1/generated/librosa.load.html]\n",
    "\n",
    "        \n",
    "        num_segments = int(np.ceil(len(y) / float(segment_length * sr))) \n",
    "        # Calculate the number of segments based on the segment length and audio length\n",
    "        \n",
    "        \n",
    "        features = [] \n",
    "        # Initialize a list to store the features for this file\n",
    "\n",
    "        \n",
    "        for i in range(num_segments): # Extracting features for each segment\n",
    "            \n",
    "            start_frame = i * segment_length * sr   # Calculate start for the current segment\n",
    "            end_frame = min(len(y), (i + 1) * segment_length * sr)    # Calculate  end frame for the current segment\n",
    "            # making sure the last frame does not excede the lenght of audio time series\n",
    "\n",
    "            \n",
    "            y_segment = y[start_frame:end_frame]# Extract audio for current segment of audio file\n",
    "\n",
    "\n",
    "            # Extract different features\n",
    "            chroma_stft = np.mean(librosa.feature.chroma_stft(y=y_segment, sr=sr))\n",
    "            # For more details : https://conference.scipy.org/proceedings/scipy2015/pdfs/brian_mcfee.pdf\n",
    "            rms = np.mean(librosa.feature.rms(y=y_segment))\n",
    "            spec_cent = np.mean(librosa.feature.spectral_centroid(y=y_segment, sr=sr))\n",
    "            spec_bw = np.mean(librosa.feature.spectral_bandwidth(y=y_segment, sr=sr))\n",
    "            rolloff = np.mean(librosa.feature.spectral_rolloff(y=y_segment, sr=sr))\n",
    "            zcr = np.mean(librosa.feature.zero_crossing_rate(y_segment))\n",
    "            mfccs = librosa.feature.mfcc(y=y_segment, sr=sr) # n_mfcc=20 by default\n",
    "            mfccs_mean = np.mean(mfccs, axis=1)\n",
    "            \n",
    "            # Append the extracted features to the list\n",
    "            features.append([chroma_stft, rms, spec_cent, spec_bw, rolloff, zcr, *mfccs_mean])\n",
    "\n",
    "        return features\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "'''Creates the seprate dataset for each of the audio file\n",
    "   with the dir name as a suffix as label\n",
    "   audio_dir = str : directory in with the file in\n",
    "   segment_length : inervel for taking samples\n",
    "   csv_output_path : path in whic you want ot store the csvs\n",
    "'''\n",
    "\n",
    "\n",
    "# Function to create the dataset\n",
    "def create_dataset_sep(audio_dir, segment_length , csv_output_path):\n",
    "    \n",
    "    labels = os.listdir(audio_dir) # Label for y\n",
    "    feature_list = []\n",
    "\n",
    "    # Iterate over all files in the audio_dir\n",
    "    for label in labels:\n",
    "        print(f'Processing {label} files...')\n",
    "        files = os.listdir(os.path.join(audio_dir, label))\n",
    "        # Wrap the files iterable with tqdm to show the progress bar\n",
    "        for file in files:\n",
    "            file_path = os.path.join(audio_dir, label, file)\n",
    "            # Extract features for the current file\n",
    "            file_features = extract_features(file_path, segment_length)\n",
    "            if file_features:\n",
    "                # Append features of all segments along with the label to the dataset\n",
    "                for segment_features in file_features:\n",
    "                    feature_list.append(segment_features + [label])\n",
    "                    \n",
    "            # Create a DataFrame with the dataset\n",
    "            df = pd.DataFrame(feature_list, columns=['chroma_stft', 'rms', 'spectral_centroid', 'spectral_bandwidth', 'rolloff', 'zero_crossing_rate', 'mfcc1', 'mfcc2', 'mfcc3', 'mfcc4', 'mfcc5', 'mfcc6', 'mfcc7', 'mfcc8', 'mfcc9', 'mfcc10', 'mfcc11', 'mfcc12', 'mfcc13', 'mfcc14', 'mfcc15', 'mfcc16', 'mfcc17', 'mfcc18', 'mfcc19', 'mfcc20','LABEL'])\n",
    "            fn_name = file.rstrip('.wav')\n",
    "            csv_output_path = f'{csv_output_path}/{fn_name}_{label}.csv'\n",
    "            df.to_csv(csv_output_path, index=False)\n",
    "\n",
    "            print(f'Dataset created and saved to {csv_output_path}')\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Function to create the dataset\n",
    "\n",
    "'''Creates the one sngle dataset for all audio file\n",
    "   with the dir name as a suffix as label\n",
    "   audio_dir = str : directory in with the file in\n",
    "   segment_length : inervel for taking samples\n",
    "'''\n",
    "\n",
    "def create_dataset(audio_dir, segment_length):\n",
    "    \n",
    "    labels = os.listdir(audio_dir) # Label for y\n",
    "    feature_list = []\n",
    "\n",
    "    # Iterate over all files in the audio_dir\n",
    "    for label in labels:\n",
    "        print(f'Processing {label} files...')\n",
    "        files = os.listdir(os.path.join(audio_dir, label))\n",
    "        # Wrap the files iterable with tqdm to show the progress bar\n",
    "        for file in files:\n",
    "            file_path = os.path.join(audio_dir, label, file)\n",
    "            # Extract features for the current file\n",
    "            file_features = extract_features(file_path, segment_length)\n",
    "            if file_features:\n",
    "                # Append features of all segments along with the label to the dataset\n",
    "                for segment_features in file_features:\n",
    "                    feature_list.append(segment_features + [label])\n",
    "                    \n",
    "    # Create a DataFrame with the dataset\n",
    "    df = pd.DataFrame(feature_list, columns=['chroma_stft', 'rms', 'spectral_centroid', 'spectral_bandwidth', 'rolloff', 'zero_crossing_rate', 'mfcc1', 'mfcc2', 'mfcc3', 'mfcc4', 'mfcc5', 'mfcc6', 'mfcc7', 'mfcc8', 'mfcc9', 'mfcc10', 'mfcc11', 'mfcc12', 'mfcc13', 'mfcc14', 'mfcc15', 'mfcc16', 'mfcc17', 'mfcc18', 'mfcc19', 'mfcc20','LABEL'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "# =============================================================================================\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "\n",
    "# Check GPU\n",
    "def check_GPU():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "    try:\n",
    "        for g in gpus:\n",
    "            tf.config.experimental.set_memory_growth(g, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print (len(gpus), 'Phusical GPUs', len(logical_gpus), 'Logical GPUs')\n",
    "    except:\n",
    "        print ('invalid device')\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "# =============================================================================================\n",
    "# ---------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract Features From Audio\n",
    "\n",
    "def create_DataFrame(File_path, segment_length):\n",
    "    feature_list =[]\n",
    "    file_features = extract_features(File_path, segment_length)\n",
    "    if file_features:\n",
    "        # Append features of all segments along with the label to the dataset\n",
    "        for segment_features in file_features:\n",
    "            feature_list.append(segment_features)\n",
    "                    \n",
    "    # Create a DataFrame with the dataset\n",
    "    df = pd.DataFrame(feature_list, columns=['chroma_stft', 'rms', 'spectral_centroid', 'spectral_bandwidth', 'rolloff', 'zero_crossing_rate', 'mfcc1', 'mfcc2', 'mfcc3', 'mfcc4', 'mfcc5', 'mfcc6', 'mfcc7', 'mfcc8', 'mfcc9', 'mfcc10', 'mfcc11', 'mfcc12', 'mfcc13', 'mfcc14', 'mfcc15', 'mfcc16', 'mfcc17', 'mfcc18', 'mfcc19', 'mfcc20'])\n",
    "    return df\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "# =============================================================================================\n",
    "# ---------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Logger\n",
    "formatter = logging.Formatter('%(asctime)s %(name)s %(levelname)s %(message)s')\n",
    "def setup_logger(name, log_file, level=logging.INFO):\n",
    "    \"\"\"To set up as many loggers as you want\"\"\"\n",
    "    handler = logging.FileHandler(log_file)\n",
    "    handler.setFormatter(formatter)\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    if logger.handlers:\n",
    "        logger.handlers = []\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "# =============================================================================================\n",
    "# ---------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
